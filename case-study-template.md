# Case-study оптимизации

## Актуальная проблема
В нашем проекте возникла серьёзная проблема.

Необходимо было обработать файл с данными, чуть больше ста мегабайт.

У нас уже была программа на `ruby`, которая умела делать нужную обработку.

Она успешно справлялась с большими файлами за 91-92 секунды, но мы решили ее дооптимизировать.

## Формирование метрики
Для того, чтобы понимать, дают ли мои изменения положительный эффект на быстродействие программы я придумал использовать такую метрику: замер времени работы скрипта и количевсто используемой памяти.

## Гарантия корректности работы оптимизированной программы
Программа поставлялась с тестом. Выполнение этого теста позволяет не допустить изменения логики программы при оптимизации.

## Feedback-Loop
Для того, чтобы иметь возможность быстро проверять гипотезы я выстроил эффективный `feedback-loop`, который позволил мне получать обратную связь по эффективности сделанных изменений за 7 секунд обработки файла.

Вот как я построил `feedback_loop`:
1. Создал shell скрипт с тремя запусками для 50k, 100k и 200k строками из data_large файла.
2. Добавил профилеровщик памяти
3. Вносил изменения в исходный код
4. Проверял скорость работы обработки файла
5. Если скорость не менялась, переходил к шагу 3. Если ускорялось, к шагу 6.
6. Переходим к следующей точке роста.

## Вникаем в детали системы, чтобы найти 20% точек роста
Для того, чтобы найти "точки роста" для оптимизации я воспользовался ruby-prof.

Вот какие проблемы удалось найти и решить

### Ваша находка №1
Используя Callstack отчет, обнаружил, что код из задания 1 имеет точку роста(12% всего времени) в методе include? для сбора уникальных браузеров.

После рефакторинга, точка роста упала до менее 2% от всего времени.
Время работы всего срипта упало с 91 секунд до 86 секунд.
Количество выделяемой памяти практически не поменялось.

### Ваша находка №2
Следующая точка роста была в Hash#to_json(около 14%) с Callstack отчетом:
14.02% (14.02%) JSON::Ext::Generator::GeneratorMethods::Hash#to_json [1 calls, 1 total]

Использование гема oj помогло ужать генерацию json до ~3%:
2.62% (2.62%) <Module::Oj>#dump [1 calls, 1 total]

### Ваша находка №3

## Результаты
В результате проделанной оптимизации наконец удалось обработать файл с данными.
Удалось улучшить метрику системы с 2.5 минут для 50_000 записей на 1.5 минут для полного файла(3_250_940 записей)

## Защита от регресса производительности
Для защиты от потери достигнутого прогресса при дальнейших изменениях программы изменен старый юнит тест, чтобы не использовать весь файл с данными, а только маленький файл для ускорения прогона тестов. Но с погрешностью в еще 1.5 минуты с учетом загруженность или других факторов на системы, которые могут повлиять на скорость обработки данных.
